{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NHW 2017. Two common pitfalls in designing of validation pipelines\n",
    "Mikhail Belyaev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### There are three main groups of feature selection methods:\n",
    " - filters\n",
    " - wrappers\n",
    " - embedded methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "% pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Filters\n",
    "\n",
    "- estimate an importance score for each feature\n",
    "- select K most important one\n",
    "- there are a lot of different ways to calculate feature importances\n",
    "- Example: http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 1. Good classification performance, but low statistical score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset1():\n",
    "    X = np.random.rand(200, 2)\n",
    "    X[:100, 0] += 1\n",
    "    X[100:150, 0] += 2\n",
    "    X[100:, 1] += 0.1\n",
    "    y = np.concatenate([np.zeros(100), np.ones(100)])\n",
    "    return X, y\n",
    "\n",
    "X, y = get_dataset1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    x_cols = ['x{}'.format(i) for i in range(X.shape[1])]\n",
    "    df = pd.DataFrame(np.hstack((X, y[:, np.newaxis])), columns=x_cols+['y'])\n",
    "    if len(x_cols) == 2:\n",
    "        sns.pairplot(df, hue='y', x_vars='x0', y_vars='x1', size=6)\n",
    "    else:\n",
    "        sns.pairplot(df, hue='y', vars=x_cols)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "selector = SelectKBest(f_classif, 1)\n",
    "selector.fit(X, y)\n",
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# mutial information\n",
    "selector = SelectKBest(mutual_info_classif, 1)\n",
    "selector.fit(X, y)\n",
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 2. Univariate stats doesn't catch bivariate dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset2(shift=0.2):\n",
    "    X = np.random.rand(1000, 3)\n",
    "    X = X[np.abs(X[:, 1] - X[:, 0]) < 0.22]\n",
    "    X = X[np.abs(X[:, 1] - X[:, 0]) > 0.02]\n",
    "    y = X[:, 1] > X[:, 0] \n",
    "    X[y, 2] += shift\n",
    "    return X, y\n",
    "X, y = get_dataset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Both univariate methods fail\n",
    "print(SelectKBest(f_classif, 1).fit(X, y).scores_)\n",
    "print(SelectKBest(mutual_info_classif, 1).fit(X, y).scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Wrappers\n",
    " - a greedy alrogithm for feature adding and/or deletion\n",
    " - there are a lot of different stratigies (starting points, criteria, etc)\n",
    " - an example http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=svc, n_features_to_select=2, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rfe.fit(X, y)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embedded \n",
    "- features are selected automatically as a part of the learning process \n",
    "- an example - linear models with the L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"width:60%; text-align:center\">\n",
    "<img src=https://1.bp.blogspot.com/-tXq6Nl2lcNg/V3qzttiZ4sI/AAAAAAAAN_M/6nmjgwydWJUy5Kqt9gFg2Nb12BCTcD4ogCLcB/s1600/LASSO.png>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l1', C=0.1)\n",
    "clf.fit(X, y)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L1-penalty based approaches are a cool class of methods, but in case of correlated variables it can drop relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us make the last component fully irrelevant and try a L1-based method again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = get_dataset2(shift=0)\n",
    "df = pd.DataFrame(np.hstack((X, y[:, np.newaxis])), columns=['x0', 'x1', 'x2', 'y'])\n",
    "sns.pairplot(df, hue='y', vars=['x0', 'x1', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l1', C=0.1)\n",
    "clf.fit(X, y)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature selection: a small example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### TODO: \n",
    " - load an anonimized dataset from dataset3.csv\n",
    " - estimate the 10 most important features (using f_classif)\n",
    " - perform cross-validation & estimate classification quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset3():\n",
    "    data = pd.read_csv('dataset3.csv')\n",
    "    y = data['y']\n",
    "    X = data.drop('y', axis=1)\n",
    "    return X, y\n",
    "X, y = get_dataset3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Data shape is {}'.format(X.shape))\n",
    "print('Class 0 size is {}'.format(sum(y==0)))\n",
    "print('Class 1 size is {}'.format(sum(y==1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The number of features is too large, so it seems to be a good idea to start with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "selector = SelectKBest(f_classif, n_features)\n",
    "X_reduced = selector.fit_transform(X, y)\n",
    "print('The new shape is {}'.format(X_reduced.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Now we have a reasonable number of features and can estimate classification accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV()\n",
    "scores = cross_val_score(clf, X_reduced, y, scoring='accuracy', cv=5)\n",
    "print('Accuracy of classification is {:0.2f} +- {:0.2f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV()\n",
    "scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "print('Accuracy of classification is {:0.2f} +- {:0.2f}'.format(np.mean(scores), np.std(scores)))\n",
    "print('Accuracy of a naive classifier is {}'.format((y==0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, we've obtained a great result using feature selection! Probably, too good to be true ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sanity check: randomly shuffle labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_random = y.copy() \n",
    "np.random.shuffle(y_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced = selector.fit_transform(X, y_random)\n",
    "scores = cross_val_score(clf, X_reduced, y_random, scoring='accuracy', cv=5)\n",
    "print('Accuracy of classification is {:0.2f} +- {:0.2f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to do a multistep analysis in a correct way? Use pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(SelectKBest(f_classif, n_features), \n",
    "                      LogisticRegressionCV())\n",
    "scores = cross_val_score(pipe, X, y_random, scoring='accuracy', cv=5)\n",
    "print('Accuracy of classification is {:0.2f} +- {:0.2f}'.format(np.mean(scores), np.std(scores)))\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, scoring='accuracy', cv=5)\n",
    "print('Accuracy of classification is {:0.2f} +- {:0.2f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selection of hyperparameters & grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def f_poly(x, coefs):\n",
    "    summands = [x**(power+1) * coef for power, coef in enumerate(coefs)]\n",
    "    return np.array(summands).sum(0)\n",
    "\n",
    "def get_function(coefs=None):\n",
    "    if coefs is None:\n",
    "        coefs = [1, -0.5, -1, 0.6]\n",
    "    return lambda x: f_poly(x, coefs)\n",
    "\n",
    "def get_dataset4(f, sample_size, noise_std=0.1):\n",
    "    X = np.random.rand(sample_size, 1) * 2 - 1\n",
    "    y = f(X)\n",
    "    y += np.random.randn(*y.shape) * noise_std\n",
    "    return X, y\n",
    "\n",
    "def plot_dataset4(f, X=None, y=None, regr=None):\n",
    "    X_plot = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
    "    plt.plot(X_plot, f(X_plot), label='True function')\n",
    "    if X is not None and y is not None:\n",
    "        plt.plot(X, y, '.r')\n",
    "    if regr is not None:\n",
    "        plt.plot(X_plot, regr.predict(X_plot), label='Prediction')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim([-0.8, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f = get_function()\n",
    "X, y = get_dataset4(f, 20)\n",
    "X_test, y_test = get_dataset4(f, 100)\n",
    "plot_dataset4(f, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "regr = make_pipeline(PolynomialFeatures(20), Ridge())\n",
    "regr.fit(X, y)\n",
    "plot_dataset4(f, X, y, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.regression import mean_squared_error as mse\n",
    "\n",
    "def get_errors(regr, X, y):\n",
    "    y_predicted = regr.predict(X)\n",
    "    mse(y, y_predicted)**0.5\n",
    "    return mse(y, y_predicted)**0.5\n",
    "print('Root mean squared error is {}'.format(get_errors(regr, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"width:100%; text-align:center\">\n",
    "<img src=http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png width=500px>\n",
    "</div>\n",
    "from a great tutorial http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We can use regularization to control model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset4(f, 20)\n",
    "regr = make_pipeline(PolynomialFeatures(20), Ridge(1e-9))\n",
    "regr.fit(X, y)\n",
    "print('Root mean squared error is {}'.format(get_errors(regr, X_test, y_test)))\n",
    "plot_dataset4(f, X, y, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset4(f, 20)\n",
    "regr = make_pipeline(PolynomialFeatures(20), Ridge(1e9))\n",
    "regr.fit(X, y)\n",
    "print('Root mean squared error is {}'.format(get_errors(regr, X_test, y_test)))\n",
    "plot_dataset4(f, X, y, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset4(f, 20)\n",
    "regr = make_pipeline(PolynomialFeatures(20), Ridge(0.1))\n",
    "regr.fit(X, y)\n",
    "print('Root mean squared error is {}'.format(get_errors(regr, X_test, y_test)))\n",
    "plot_dataset4(f, X, y, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to select model parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'ridge__alpha':10**np.linspace(-5, 5, 21)}\n",
    "regr = make_pipeline(PolynomialFeatures(20), Ridge())\n",
    "regr_grid = GridSearchCV(regr, parameters)\n",
    "regr_grid.fit(X, y)\n",
    "\n",
    "plot_dataset4(f, X, y, regr_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### But what is we have more than one parameter to adjust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "parameters = {'learning_rate': [0.01, 0.025, 0.05, 0.1],\n",
    "              'max_depth': [1, 2, 3, 4, 5],\n",
    "              'n_estimators': [10, 20, 30, 50]\n",
    "             }\n",
    "clf_grid = GridSearchCV(clf, parameters, n_jobs=-1, verbose=True)\n",
    "X, y = get_dataset3()\n",
    "clf_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An error with a fixed parameters is a random variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = get_dataset4(f, 20)\n",
    "regr = make_pipeline(PolynomialFeatures(20), Ridge())\n",
    "regr.fit(X, y)\n",
    "print('Root mean squared error is {}'.format(get_errors(regr, X_test, y_test)))\n",
    "plot_dataset4(f, X, y, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_scores(regr, sample_size, n_repeats):\n",
    "    scores = []\n",
    "    for i in range(n_repeats):\n",
    "        X, y = get_dataset4(f, sample_size)\n",
    "        regr.fit(X, y)\n",
    "        scores.append(get_errors(regr, X_test, y_test))\n",
    "    return scores\n",
    "regr = make_pipeline(PolynomialFeatures(20), Ridge())\n",
    "scores = get_scores(regr, 20, 100)\n",
    "sns.distplot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
    "sns.despine(left=True)\n",
    "\n",
    "def plot_hist(alpha, color, ax):\n",
    "    regr = make_pipeline(PolynomialFeatures(20), Ridge(alpha=alpha))\n",
    "    scores = get_scores(regr, 20, 100)\n",
    "    sns.distplot(scores, color=color, ax=ax)\n",
    "    ax.set_title('Regularization is {}'.format(alpha))\n",
    "\n",
    "plot_hist(alpha=1e-9, color='r', ax=axes[0, 0])\n",
    "plot_hist(alpha=1e-4, color='g', ax=axes[0, 1])\n",
    "plot_hist(alpha=1e-1, color='b', ax=axes[1, 0])\n",
    "plot_hist(alpha=1e9, color='m', ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take-away messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Ideally, use an independent test set \n",
    " - If you use multistep anysis always chain these steps into a single sklearn pipeline\n",
    " - As a sanity check, you can feed to your analysis random variables and compare the obrained results with the expected quality of random classification\n",
    " - Do not trust GridSearchCV results, always re-check the optimal comination of parameters"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
